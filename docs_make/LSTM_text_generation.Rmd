---
title: "LSTM Text Generation"
output:
  html_document:
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: true
    code_folding: show
---

```{r, echo=FALSE, message=FALSE}

require("readr")
require("dplyr")
require("plotly")
require("stringr")
require("stringi")
require("mxnet")

source("../mx_io_bucket_iter.R")
source("../rnn_bucket_setup_Dev.R")
source("../rnn_bucket_train_Dev.R")

```


Approach: 

1. Data preparation
  1.1. Read a csv file  
  1.2. Split into lines  
  1.3. Encode into integrers

2. Iterator  
    2.1 Iterator that return bucket ID.  
    2.2 Needs to sample from batches that share the same bucket ID.

Retained solution: create a list of arrays, each array corresponds to a bucket. 

Idea is to invest time in preperation rather than during the training

## Build a dictionnary 

```{r, echo=TRUE}

make_dic <- function(text, max.vocab=10000) {
  
  dic_labels <- sort(unique(text))
  #dic <- 1:length(dic_labels)-1
  dic <- 1:length(dic_labels)
  names(dic)<- dic_labels
  
  cat(paste0("Total unique char: ", length(dic), "\n"))
  return (dic)
}

```


## Index data to vocabulary

```{r, echo=TRUE}

make_data <- function(path, seq.len=32, max.vocab=10000, dic=NULL) {
  
  text_vec <- read_file(file = path)
  text_vec <- strsplit(text_vec, '') %>% unlist
  
  if (is.null(dic)){
    dic <- make_dic(text_vec, max.vocab)
  }
  
  rev_dic<- names(dic)
  names(rev_dic)<- dic
  
  ### Adjuste by -1 because need a 1-lag for labels
  num.seq <- as.integer(floor((length(text_vec)-1)/seq.len))
  
  features<- dic[text_vec[1:(seq.len*num.seq)]] 
  labels<- dic[text_vec[1:(seq.len*num.seq)+1]]
  
  features_array <- array(features, dim=c(seq.len, num.seq))
  labels_array <- array(labels, dim=c(seq.len, num.seq))
  
  return (list(features_array=features_array, labels_array=labels_array, dic=dic, rev_dic=rev_dic))
}


### Altenative avenues to consider for breaking into characters, words or characters
# stri_split_boundaries("alfdsl fdslkj fds Tre Alo. BOn bewn", type="sentence", simplify = T)
# stri_split_boundaries("alfdsl fdslkj fds Tre Alo. BOn bewn", type="character", simplify = T)


```


###Array List Bucket Iterator

A bucketID is returned at initialisation - required for the initial setup the executor. 

During training, the executor is reloaded through a binding to the next executor

## Prepare the Obama speech data

```{r, echo=TRUE}

seq.len<- 100
system.time(data_prep <- make_data(path = "../data/obama.txt", seq.len=seq.len, dic=NULL))

X <- data_prep$features_array
Y <- data_prep$labels_array
dic <- data_prep$dic
rev_dic <- data_prep$rev_dic
vocab <- length(dic)

shape <- dim(X)
train.val.fraction <- 0.9
size <- shape[2]

X.train.data <- X[, 1:as.integer(size * train.val.fraction)]
X.val.data <- X[, -(1:as.integer(size * train.val.fraction))]

X.train.label <- Y[, 1:as.integer(size * train.val.fraction)]
X.val.label <- Y[, -(1:as.integer(size * train.val.fraction))]

```


## Visualise the LSTM network

Modification brought to the original model: 

  - Use a list of symbols, one for every bucket, instead of a single unrolled LSTM

```{r, fig.height=10, echo=TRUE}

test_symbol <- rnn.unroll(num.rnn.layer = 1,
                          seq.len = 2, 
                          input.size=vocab,
                          num.hidden=24,
                          num.embed=32,
                          num.label=vocab,
                          dropout=0, config = "one-to-one", 
                          ignore_label = 0)

test_symbol$arguments
graph.viz(test_symbol, type="graph", direction="UD")

```


## Evaluation metrics

```{r, echo=TRUE}

mx.metric.custom<- function (name, feval) 
{
  init <- function() {
    c(0, 0)
  }
  update <- function(label, pred, state) {
    m <- feval(label, pred)
    state <- c(state[[1]] + 1, state[[2]] + m)
    return(state)
  }
  get <- function(state) {
    list(name = name, value = (state[[2]]/state[[1]]))
  }
  ret <- (list(init = init, update = update, get = get))
  class(ret) <- "mx.metric"
  return(ret)
}

mx.metric.Perplexity <- mx.metric.custom("Perplexity", function(label, pred){
  label_probs <- as.array(mx.nd.choose.element.0index(pred, label))
  batch <- length(label_probs)
  NLL <- -sum(log(pmax(1e-15, as.array(label_probs)))) / batch
  Perplexity <- exp(NLL)
  return(Perplexity)
})

```


## Train a language model 

```{r, echo=TRUE, eval=FALSE}

### Create iterators
batch_size = 64

train_buckets<- list("100"=list(data=X.train.data, label=X.train.label))
eval_buckets<- list("100"=list(data=X.val.data, label=X.val.label))

iter_train<- mx_io_bucket_iter(buckets = train_buckets, batch_size = batch_size, data_mask_element = 0, shuffle = T)

iter_eval<- mx_io_bucket_iter(buckets = eval_buckets, batch_size = batch_size, data_mask_element = 0, shuffle = T)

ctx<- list(mx.cpu())

dropout = 0
num.rnn.layer = 1
num.hidden = 32
num.embed = 16
num.label = vocab 
input.size = vocab
metric = mx.metric.Perplexity
kvstore="local"
batch.end.callback<- NULL
epoch.end.callback<- NULL
#optimizer<- mx.opt.create("sgd", learning.rate=0.02, wd=0.0001, rescale.grad=1/batch_size)
optimizer<- mx.opt.create("adadelta", rho=0.92, eps=1e-6, wd=0.0001, rescale.grad=1/batch_size)
initializer=mx.init.uniform(0.1)
config = "one-to-one"
verbose=T

system.time(model_obama_1<- mx.rnn.buckets(train.data = iter_train, 
                               eval.data=iter_eval,
                               num.rnn.layer = 1, 
                               num.hidden = 50, 
                               num.embed = 32, 
                               num.label = vocab, 
                               input.size = vocab, 
                               ctx = ctx, 
                               begin.round = 1,
                               end.round = 5,
                               initializer = initializer, 
                               dropout = 0.2, 
                               metric = mx.metric.Perplexity, 
                               config = "one-to-one", 
                               optimizer = optimizer,
                               verbose=TRUE)
)

mx.model.save(model = model_obama_1, prefix = "../models/obama_test1", iteration = 5)

```


## Inference from trained model

### Convert new inputs into buckets

```{r}

make_inference_data <- function(text, max.vocab=10000, dic) {
  
  text_vec <- strsplit(text, '') %>% unlist
  
  seq.len<- length(text_vec)
  
  rev_dic<- names(dic)
  names(rev_dic)<- dic
  
  ### Adjust by -1 because need a 1-lag for labels
  num.seq <- as.integer(ceiling((length(text_vec)-1)/seq.len))
  
  features<- dic[text_vec[1:(seq.len*num.seq)]] 
  labels<- dic[text_vec[1:(seq.len*num.seq)+1]]
  
  features[is.na(features)]<- 0
  labels[is.na(labels)]<- 0
  
  features_array <- array(features, dim=c(seq.len, num.seq))
  labels_array <- array(labels, dim=c(seq.len, num.seq))
  
  return (list(features_array=features_array, labels_array=labels_array, dic=dic, rev_dic=rev_dic))
}

infer_prep<- make_inference_data(text = "The United States", dic = data_prep$dic)

infer_features <- infer_prep$features_array
infer_labels <- infer_prep$labels_array
dic <- data_prep$dic
rev_dic <- data_prep$rev_dic
vocab <- length(dic)

shape <- dim(infer_features)
size <- shape[2]

infer_buckets<- list(list(data=infer_features, label=infer_labels))
names(infer_buckets)<- paste0(shape[1])


infer_iter<- mx_io_bucket_iter(buckets = infer_buckets, batch_size = 1, data_mask_element = 0, shuffle = F)

```

### Generate inference

```{r, echo=TRUE, eval=FALSE}

model_obama<- mx.model.load(prefix = "../models/obama_test1", iteration = 5)

### Get Initial Inference from sentences
infer_init<- mx.rnn.infer.buckets(infer_iter = infer_iter,
                                  model = model_obama,
                                  config = "one-to-one",
                                  ctx = ctx, 
                                  output_last_state=TRUE,
                                  kvstore = NULL)

### Generate muultiple predictions from a given initial state
infer_recur<- mx.rnn.infer.buckets(infer_iter = infer_iter,
                                   model = model_obama,
                                   config = "one-to-one",
                                   ctx = ctx, 
                                   output_last_state=TRUE,
                                   init.state = infer_init[1:(length(infer_init)-1)]
                                   kvstore = NULL)



preds<- infer_test$pred
labels<- infer_test$label


```

