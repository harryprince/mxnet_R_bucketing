---
title: "Text Generation with RNN"
output:
  html_document:
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: true
    code_folding: show
---

```{r, include=FALSE}

require("readr")
require("dplyr")
require("plotly")
require("stringr")
require("stringi")
require("mxnet")

#source("rnn_model_bucket_full.R")
#source("lstm_bucket_full.R")

source("../mx_io_bucket_iter.R")
source("../rnn_bucket_setup_Dev.R")
source("../rnn_bucket_train_Dev.R")

```


Approach: 

1. Data preparation
  1.1. Read a csv file  
  1.2. Split into lines  
  1.3. Encode into integrers

2. Iterator  
    2.1 Iterator that return bucket ID.  
    2.2 Needs to sample from batches that share the same bucket ID.

Retained solution: create a list of arrays, each array corresponds to a bucket. 

Idea is to invest time in preperation rather than during the training

###Build a dictionnary 

```{r, echo=T}

make_dic <- function(text, max.vocab=10000) {
  
  dic_labels <- sort(unique(text))
  #dic <- 1:length(dic_labels)-1
  dic <- 1:length(dic_labels)
  names(dic)<- dic_labels
  
  cat(paste0("Total unique char: ", length(dic), "\n"))
  return (dic)
}

```


### Indexing data to vocabulary

```{r}

make_data <- function(path, seq.len=32, max.vocab=10000, dic=NULL) {
  
  text_vec <- read_file(file = path)
  text_vec <- strsplit(text_vec, '') %>% unlist
  
  if (is.null(dic)){
    dic <- make_dic(text_vec, max.vocab)
  }
  
  rev_dic<- names(dic)
  names(rev_dic)<- dic
  
  ### Adjuste by -1 because need a 1-lag for labels
  num.seq <- as.integer(floor((length(text_vec)-1)/seq.len))
  
  features<- dic[text_vec[1:(seq.len*num.seq)]] 
  labels<- dic[text_vec[1:(seq.len*num.seq)+1]]
  
  features_array <- array(features, dim=c(seq.len, num.seq))
  labels_array <- array(labels, dim=c(seq.len, num.seq))
  
  return (list(features_array=features_array, labels_array=labels_array, dic=dic, rev_dic=rev_dic))
}


###
# stri_split_boundaries("alfdsl fdslkj fds Tre Alo. BOn bewn", type="sentence", simplify = T)
# stri_split_boundaries("alfdsl fdslkj fds Tre Alo. BOn bewn", type="character", simplify = T)


```



###Array List Bucket Iterator

A bucketID is returned at initialisation - required for the initial setup the executor. 

During training, the executor is reloaded through a binding to the next executor

###Prepare the Obama speech data

```{r}

batch.size = 32
seq.len = 100
num.hidden = 128
num.embed = 32
num.lstm.layer = 1
num.round = 1
learning.rate= 0.05
wd=0.001
clip_gradient=1
update.period = 1


system.time(data_prep <- make_data(path = "../data/obama.txt", seq.len=seq.len, dic=NULL))

X <- data_prep$features_array
Y <- data_prep$labels_array
dic <- data_prep$dic
rev_dic <- data_prep$rev_dic
vocab <- length(dic)

shape <- dim(X)
train.val.fraction <- 0.9
size <- shape[2]

X.train.data <- X[, 1:as.integer(size * train.val.fraction)]
X.val.data <- X[, -(1:as.integer(size * train.val.fraction))]

X.train.label <- Y[, 1:as.integer(size * train.val.fraction)]
X.val.label <- Y[, -(1:as.integer(size * train.val.fraction))]

### Create iterators
batch_size = 64

train_buckets<- list("100"=list(data=X.train.data, label=X.train.label))
eval_buckets<- list("100"=list(data=X.val.data, label=X.val.label))

X_iter_train<- mx_io_bucket_iter(buckets = train_buckets, batch_size = batch_size, data_mask_element = 0, shuffle = T)

X_iter_eval<- mx_io_bucket_iter(buckets = eval_buckets, batch_size = batch_size, data_mask_element = 0, shuffle = T)

# X_iter_train$init()
# X_iter_train$reset()
# X_iter_train$iter.next()
# X_iter_train$value()

```


###Run a bucketed LSTM model

Modification brought to the original model: 

  - Use a list of symbols, one for every bucket, instead of a single unrolled LSTM
  

```{r, fig.height=10}

test_symbol <- rnn.unroll(num.rnn.layer = num.lstm.layer,
                          seq.len = 2, 
                          input.size=vocab,
                          num.hidden=num.hidden,
                          num.embed=num.embed,
                          num.label=vocab,
                          dropout=0, config = "one-to-one", 
                          ignore_label = 0)

test_symbol$arguments
graph.viz(test_symbol, type="graph", direction="UD")
#graph.viz(test_symbol, type="vis", direction="UD")

```


### Evaluation metrics

```{r}

calc.nll <- function(seq.label.probs, batch.size) {
  nll = - sum(log(seq.label.probs)) / batch.size
  return (nll)
}


mx.metric.NLL <- mx.metric.custom("NLL", function(label, pred){
  
  label_probs <- mx.nd.choose.element.0index(pred, label)
  batch <- length(label_probs)
  NLL <- -sum(log(pmax(1e-15, label_probs))) / batch
  #Perplexity <- (1/batch) * sum(abs(label-pred))
  return(NLL)
})


mx.metric.Perplexity <- mx.metric.custom("Perplexity", function(label, pred){
  
  label_probs <- mx.nd.choose.element.0index(pred, label)
  batch <- length(label_probs)
  NLL <- -sum(log(pmax(1e-15, label_probs))) / batch
  Perplexity <- exp(NLL/100)
  return(Perplexity)
})


```


```{r}

ctx<- list(mx.cpu())

dropout = 0
num.rnn.layer = 1
num.hidden = 32
num.embed = 16
num.label = vocab 
input.size = vocab
metric = mx.metric.Perplexity
kvstore="local"
batch.end.callback<- NULL
epoch.end.callback<- NULL
optimizer<- mx.opt.create("sgd", learning.rate=learning.rate, wd=wd, rescale.grad=1/32)
initializer=mx.init.uniform(0.1)

system.time(model_obama_1<- mx.rnn.buckets(train.data = X_iter_train, 
                               eval.data=NULL,
                               num.rnn.layer = 1, 
                               num.hidden = 32, 
                               num.embed = 16, 
                               num.label = vocab, 
                               input.size = vocab, 
                               ctx = ctx, 
                               begin.round = 1,
                               end.round = 1,
                               initializer = initializer, 
                               dropout = 0, 
                               metric = NULL, 
                               config = "one-to-one", 
                               optimizer = optimizer,
                               verbose=TRUE)
)

model_test1<- mx.model.train.rnn.buckets(symbol = symbol, 
                                         input.shape = c(seq.len, batch.size), 
                                         arg.params = params$arg.params, 
                                         aux.params = params$aux.params, begin.round = 1, 
                                         end.round = 1, 
                                         ctx = ctx, 
                                         train.data =  X_iter_train, 
                                         #metric = mx.metric.NLL, 
                                         optimizer = optimizer, 
                                         kvstore = NULL,
                                         batch.size = batch.size, 
                                         batch.end.callback=mx.callback.log.train.metric(period = 10))

model <- mx.lstm.new(train.data = X_iter_train, 
                     eval.data = X_iter_eval, 
                     ctx=devices,
                     num.round=2, 
                     update.period=update.period,
                     num.lstm.layer=num.lstm.layer, 
                     seq.len=seq.len,
                     num.hidden=num.hidden, 
                     num.embed=num.embed, 
                     num.label=vocab,
                     batch.size=batch.size, 
                     input.size=vocab,
                     #initializer=mx.init.uniform(0.1), 
                     initializer=mx.init.Xavier(rnd_type = "gaussian", factor_type = "in", magnitude = 2), 
                     learning.rate=learning.rate,
                     wd=wd,
                     clip_gradient=clip_gradient)

```


