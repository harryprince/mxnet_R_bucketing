---
title: "Bucketing with MXNet for R"
date: "5 f√©vrier 2017"
output: 
    html_document: 
      toc: true
---

```{r, include=FALSE}

require("readr")
require("dplyr")
require("plotly")
require("stringr")
require("stringi")
require("mxnet")

#source("rnn_model_bucket_full.R")
source("lstm_bucket_full.R")

```


Approach: 

1. Data preparation
  1.1. Read a csv file  
  1.2. Split into lines  
  1.3. Encode into integrers

2. Iterator  
    2.1 Iterator that return bucket ID.  
    2.2 Needs to sample from batches that share the same bucket ID.

Retained solution: create a list of arrays, each array corresponds to a bucket. 

Idea is to invest time in preperation rather than during the training

###Build a dictionnary 

```{r, echo=T}

make_dic <- function(text, max.vocab=10000) {
  
  dic_labels <- sort(unique(text))
  dic <- 1:length(dic_labels)-1
  names(dic)<- dic_labels
  
  cat(paste0("Total unique char: ", length(dic), "\n"))
  return (dic)
}

```


### Indexing data to vocabulary

```{r}

make_data <- function(path, seq.len=32, max.vocab=10000, dic=NULL) {
  
  text_vec <- read_file(file = path)
  text_vec <- strsplit(text_vec, '') %>% unlist
  
  if (is.null(dic)){
    dic <- make_dic(text_vec, max.vocab)
  }
  
  rev_dic<- names(dic)
  names(rev_dic)<- dic
  
  ### Adjuste by -1 because need a 1-lag for labels
  num.seq <- as.integer(floor((length(text_vec)-1)/seq.len))
  
  features<- dic[text_vec[1:(seq.len*num.seq)]] 
  labels<- dic[text_vec[1:(seq.len*num.seq)+1]]
  
  features_array <- array(features, dim=c(seq.len, num.seq))
  labels_array <- array(labels, dim=c(seq.len, num.seq))
  
  return (list(features_array=features_array, labels_array=labels_array, dic=dic, rev_dic=rev_dic))
}


###
stri_split_boundaries("alfdsl fdslkj fds Tre Alo. BOn bewn", type="sentence", simplify = T)
stri_split_boundaries("alfdsl fdslkj fds Tre Alo. BOn bewn", type="character", simplify = T)


```



###Array List Bucket Iterator


A bucketID is returned at initialisation - required for the initial setup the executor. 

During training, the executor is reloaded through a binding to the next executor

```{r}

data_train1<- array(data = runif(20), dim = c(2,10))
target_train1<- data_train1

data_train2<- array(data = runif(30), dim = c(2,15))
target_train2<- data_train2

data_train<- list("10"=data_train1, "15"=data_train2)
target_train<- list("10"=target_train1, "15"=target_train2)

R_iter<- function(data_list, label_list, batch_size){
  
  buckets<- names(data_list)
  buckets_nb<- length(buckets)
  buckets_id<- 1:buckets_nb
  
  buckets_size<- sapply(data_list, function(x) last(dim(x)))
  batch_per_bucket<- floor(buckets_size/batch_size)
  
  ### Number of batches per epoch given the batch_size
  batch_per_epoch<- sum(batch_per_bucket)
  
  epoch<- 0
  batch<- 0
  
  bucket_plan<- NULL
  bucketID<-NULL

  bucket_plan_fun<- function(){
    bucketID<<-batch
    return(bucketID)
  }  
  
  bucket_fun<- function(){
    bucketID<- bucketID
    return(bucketID)
  }
  
  reset<- function(){
    epoch<<- epoch+1
    batch<<- 0
    bucket_plan_names<- sample(rep(names(batch_per_bucket), times=batch_per_bucket))
    bucket_plan<<- ave(bucket_plan_names==bucket_plan_names, bucket_plan_names, FUN=cumsum)
    names(bucket_plan)<<- bucket_plan_names
    
    ### Return first BucketID at reset for initialization of the model
    bucketID<<- bucket_plan[1]
    bucket_fun()
    
    return(!is.null(bucket_plan))
  }
  
  iter.next<- function(){
    batch<<- batch+1
    bucketID<<- bucket_plan[batch]
    bucket_fun()
    if (batch>batch_per_epoch) {
      return(FALSE)
    } else {
      return(TRUE)
    }
  }
  
  value<- function(){
    
    ### bucketID is a named integer whose value indicates the batch nb for the given bucket and the name is the a character string indicating the sequence length of the bucket
    idx<- (bucketID-1)*(batch_size)+(1:batch_size)
    data<- data_list[[names(bucketID)]][,idx]
    label<- label_list[[names(bucketID)]][,idx]
    return(list(data=mx.nd.array(data), label=mx.nd.array(label)))
  }
  
  return(list(reset=reset, iter.next=iter.next, value=value, bucketID=bucket_fun, buckets=buckets))
}

test_bucket_iter<- R_iter(data_list = data_train, label_list = target_train, batch_size = 4)

test_bucket_iter$buckets
test_bucket_iter$reset()
test_bucket_iter$bucketID()
test_bucket_iter$iter.next()
test_bucket_iter$value()
test_bucket_iter$bucketID()

```

###Prepare the Obama speech data

```{r}

batch.size = 32
seq.len = 100
num.hidden = 128
num.embed = 32
num.lstm.layer = 1
num.round = 1
learning.rate= 0.05
wd=0.001
clip_gradient=1
update.period = 1


system.time(data_prep <- make_data(path = "../data/obama.txt", seq.len=seq.len, dic=NULL))

X <- data_prep$features_array
Y <- data_prep$labels_array
dic <- data_prep$dic
rev_dic <- data_prep$rev_dic
vocab <- length(dic)

shape <- dim(X)
train.val.fraction <- 0.9
size <- shape[2]

X.train.data <- X[, 1:as.integer(size * train.val.fraction)]
X.val.data <- X[, -(1:as.integer(size * train.val.fraction))]

X.train.label <- Y[, 1:as.integer(size * train.val.fraction)]
X.val.label <- Y[, -(1:as.integer(size * train.val.fraction))]

X.train <- list(data=X.train.data, label=X.train.label)
X.val <- list(data=X.val.data, label=X.val.label)

X_iter_train<- R_iter(data_list = list("100"=X.train.data), label_list = list("100"=X.train.label), batch_size = batch.size)

X_iter_eval<- R_iter(data_list = list("100"=X.val.data), label_list = list("100"=X.val.label), batch_size = batch.size)

```


###Run a bucketed LSTM model

Modification brought to the original model: 

  - Use a list of symbols, one for every bucket, instead of a single unrolled LSTM
  

```{r, fig.height=10}

test_symbol <- mxnet:::lstm.unroll(num.lstm.layer=num.lstm.layer,
                            seq.len = 2, 
                            input.size=vocab,
                            num.hidden=num.hidden,
                            num.embed=num.embed,
                            num.label=vocab,
                            dropout=0)

graph.viz(test_symbol, type="vis", direction="UD")


```


```{r}


calc.nll <- function(seq.label.probs, batch.size) {
  nll = - sum(log(seq.label.probs)) / batch.size
  return (nll)
}


mx.metric.NLL <- mx.metric.custom("NLL", function(label, pred){
  
  label_probs <- mx.nd.choose.element.0index(pred, label)
  batch <- length(label_probs)
  NLL <- -sum(log(pmax(1e-15, label_probs))) / batch
  #Perplexity <- (1/batch) * sum(abs(label-pred))
  return(NLL)
})


mx.metric.Perplexity <- mx.metric.custom("Perplexity", function(label, pred){
  
  label_probs <- mx.nd.choose.element.0index(pred, label)
  batch <- length(label_probs)
  NLL <- -sum(log(pmax(1e-15, label_probs))) / batch
  Perplexity <- exp(NLL/100)
  return(Perplexity)
})


```


```{r}

ctx<- list(mx.cpu())

train.data = X_iter_train
eval.data = X_iter_eval
num.round=2
num.label=vocab
input.size=vocab
initializer=mx.init.uniform(0.1)
dropout=0
verbose=TRUE
kvstore="local"
metric<- mx.metric.rmse
batch.end.callback<- NULL
epoch.end.callback<- NULL
optimizer<- mx.opt.create("sgd", learning.rate=learning.rate, wd=wd, rescale.grad=1/32)
begin.round=1
end.round=2

model_test1<- mx.model.train.rnn(symbol = symbol, 
                                 input.shape = c(seq.len, batch.size), 
                                 arg.params = params$arg.params, 
                                 aux.params = params$aux.params, begin.round = 1, 
                                 end.round = 1, 
                                 ctx = ctx, 
                                 train.data =  X_iter_train, 
                                 #metric = mx.metric.NLL, 
                                 optimizer = optimizer, 
                                 kvstore = NULL,
                                 batch.size = batch.size, 
                                 batch.end.callback=mx.callback.log.train.metric(period = 10))

model <- mx.lstm.new(train.data = X_iter_train, 
                 eval.data = X_iter_eval, 
                 ctx=devices,
                 num.round=2, 
                 update.period=update.period,
                 num.lstm.layer=num.lstm.layer, 
                 seq.len=seq.len,
                 num.hidden=num.hidden, 
                 num.embed=num.embed, 
                 num.label=vocab,
                 batch.size=batch.size, 
                 input.size=vocab,
                 #initializer=mx.init.uniform(0.1), 
                 initializer=mx.init.Xavier(rnd_type = "gaussian", factor_type = "in", magnitude = 2), 
                 learning.rate=learning.rate,
                 wd=wd,
                 clip_gradient=clip_gradient)

```


