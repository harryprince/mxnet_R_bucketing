---
title: "Time-Series on CPU"
output: github_document
editor_options: 
  chunk_output_type: inline
---

This is a minimalistic demo on how to model time-series with RNN, including training and inference. 

It relies on an explicit symbolic construction of the RNN cells (LSTM or GRU), thus supporting training on CPU unlike the FusedRNN operator that is CUDA dependent. 

```{r, echo=TRUE, message=FALSE}
library("readr")
library("dplyr")
library("plotly")
library("mxnet")
```

### Data preparation

In this step a collection of univariate time-series is built. 
192 independent times series (_samples_) with each 100 observations (_seq_len_) are created. The modeling task will is predict the next value given the previous ones. 



The target variable is therefore obtained by shifting the features by one timestep in the future. 

```{r, echo=TRUE}
# number of timestamps
seq_len = 100

# nunmber of samples
n = 500

# return a random starting point of the time series
set.seed(12)
seeds <- runif(n, min = 0, max = 24)

# generate the time series of seq_length for each starting point
pts <- sapply(seeds, function(x) sin(x + pi/12 * (0:(seq_len))))

# build the features matrix
x <- pts[1:seq_len, ]
x <- array(x, dim = c(1, seq_len, n))

# build the target array - same as feature but one timestep forward
y <- pts[-1, ]

# plot time series from 5 first samples
plot_ly(x = 1:dim(x)[2], y = x[,,1], type = "scatter", mode = "lines") %>% 
  add_trace(x = 1:dim(x)[2], y = x[,,2], type = "scatter", mode = "lines") %>% 
  add_trace(x = 1:dim(x)[2], y = x[,,3], type = "scatter", mode = "lines") %>% 
  add_trace(x = 1:dim(x)[2], y = x[,,4], type = "scatter", mode = "lines") %>% 
  add_trace(x = 1:dim(x)[2], y = x[,,5], type = "scatter", mode = "lines") %>% 
  layout(title = "Sample of 5 time series")


# plot first time series and its associated target
plot_ly(x = 1:dim(x)[2], y = x[,,1], type = "scatter", mode = "lines", name = "feature") %>% 
  add_trace(x = 1:dim(x)[2], y = y[,1], type = "scatter", mode = "lines", name = "target") %>% 
  layout(title = "Sample of one time series")
```

### Create data iterators

The training and evaluation data are made by splitting 

```{r}
batch.size = 32

# take first 400 samples for train - remaining 100 for evaluation
train_ids <- 1:400

train.data <- mx.io.arrayiter(data = x[,,train_ids, drop = F], label = y[, train_ids], 
                              batch.size = batch.size, shuffle = TRUE)

eval.data <- mx.io.arrayiter(data = x[,,-train_ids, drop = F], label = y[, -train_ids], 
                              batch.size = batch.size, shuffle = FALSE)
```


### Model architecture

```{r, echo=TRUE}
symbol <- rnn.graph.unroll(seq_len = 2, 
                           num_rnn_layer =  1, 
                           num_hidden = 50,
                           input_size = NULL,
                           num_embed = NULL, 
                           num_decode = 1,
                           masking = F, 
                           loss_output = "linear",
                           dropout = 0.2, 
                           ignore_label = -1,
                           cell_type = "lstm",
                           output_last_state = F,
                           config = "one-to-one")

graph = graph.viz(symbol, type = "graph", direction = "TD", 
                  shape=list(data = c(1, 2, batch.size), 
                             label = c(2, batch.size)))
```

```{r, echo = FALSE, fig.height=12}
# DiagrammeRsvg::export_svg(graph) %>% charToRaw %>% rsvg::rsvg_png("time_graph.png")
graph
```


```{r, echo = FALSE}
symbol <- rnn.graph.unroll(seq_len = seq_len, 
                           num_rnn_layer =  2, 
                           num_hidden = 50,
                           input_size = NULL,
                           num_embed = NULL, 
                           num_decode = 1,
                           masking = F, 
                           loss_output = "linear",
                           dropout = 0.1, 
                           ignore_label = -1,
                           cell_type = "lstm",
                           output_last_state = F,
                           config = "one-to-one")
```


### Fit a LSTM model

Needs a custom metric to handle labels in a matrix rather than flat format in the iterator. 

```{r}
mx.metric.mse.seq <- mx.metric.custom("MSE", function(label, pred) {
  label = mx.nd.reshape(label, shape = -1)
  label = as.array(label)
  pred = as.array(pred)
  res <- mean((label-pred)^2)
  return(res)
})
```


```{r, echo=TRUE, eval=TRUE, message=FALSE}
ctx <- mx.cpu()

initializer <- mx.init.Xavier(rnd_type = "gaussian", 
                              factor_type = "avg", 
                              magnitude = 2.5)

optimizer <- mx.opt.create("adadelta", rho = 0.9, eps = 1e-5, wd = 0,
                           clip_gradient = 1, rescale.grad = 1/batch.size)

logger <- mx.metric.logger()
epoch.end.callback <- mx.callback.log.train.metric(period = 10, logger = logger)

system.time(
  model <- mx.model.buckets(symbol = symbol,
                            train.data = train.data, 
                            eval.data = eval.data, 
                            num.round = 100, ctx = ctx, verbose = TRUE,
                            metric = mx.metric.mse.seq, 
                            initializer = initializer, optimizer = optimizer, 
                            batch.end.callback = NULL, 
                            epoch.end.callback = epoch.end.callback)
)
```

```{r, fig.height=5}
plot_ly(x = seq_len(length(logger$train)), y = logger$train, type = "scatter", 
        mode = "markers+lines", name = "train") %>% 
  add_trace(y = logger$eval, type = "scatter", 
            mode = "markers+lines", name = "eval")
```


## Inference on test data

Setup inference data. Need to apply preprocessing to inference sequence and convert into a infer data iterator. 

### Build the inference symbols

Two symbols: 

- first unrolled to the length of the inference sequence. 
- second of length one on which to iterate up to the number of the desired inference steps. 

```{r}
ctx <- mx.cpu()

pred_length = 80
data = mx.nd.array(x[, , 1, drop = F])
infer_length_ini <- dim(data)[2]

symbol.infer.ini <- rnn.graph.unroll(seq_len = infer_length_ini,
                                     num_rnn_layer = 2, 
                                     num_hidden = 50,
                                     input_size = NULL,
                                     num_embed = NULL, 
                                     num_decode = 1,
                                     masking = F, 
                                     loss_output = "linear",
                                     dropout = 0, 
                                     ignore_label = -1,
                                     cell_type = "lstm",
                                     output_last_state = T,
                                     config = "one-to-one")

symbol.infer <- rnn.graph.unroll(seq_len = 1,
                                 num_rnn_layer = 2, 
                                 num_hidden = 50,
                                 input_size = NULL,
                                 num_embed = NULL, 
                                 num_decode = 1,
                                 masking = F, 
                                 loss_output = "linear",
                                 dropout = 0, 
                                 ignore_label = -1,
                                 cell_type = "lstm",
                                 output_last_state = T,
                                 config = "one-to-one")
```


### Inference

```{r, echo=TRUE, eval=TRUE}
predict <- numeric()

infer <- mx.infer.rnn.one.unroll(infer.data = data, 
                                 symbol = symbol.infer.ini,
                                 num_hidden = 50,
                                 arg.params = model$arg.params,
                                 aux.params = model$aux.params,
                                 init_states = NULL,
                                 ctx = ctx)

pred = mx.nd.array(y[seq_len, 1, drop = F])
real = sin(seeds[1] + pi/12 * (seq_len+1):(seq_len+pred_length))

for (i in 1:pred_length) {
  
  data = mx.nd.reshape(pred, shape = c(1,1,1))
  
  infer <- mx.infer.rnn.one.unroll(infer.data = data, 
                                   symbol = symbol.infer,
                                   num_hidden = 50,
                                   arg.params = model$arg.params,
                                   aux.params = model$aux.params,
                                   init_states = infer[-1],
                                   ctx = ctx)
  
  pred <- infer[[1]]
  predict <- c(predict, as.numeric(as.array(pred)))
}
```

### Plot predictions against real values

```{r}
data = mx.nd.array(x[, , 1, drop = F])
label = mx.nd.array(y[, 1, drop = F])

plot_ly(x = 1:dim(y)[1], y = y[,1], type = "scatter", mode="lines", name = "hist") %>% 
  add_trace(x = dim(y)[1] + 1:length(real), y = real, type = "scatter", mode="lines", name = "real") %>% 
  add_trace(x = dim(y)[1] + 1:length(predict), y = predict, type = "scatter", mode="lines", name = "pred")
```

