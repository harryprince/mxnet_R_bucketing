---
title: "NLP Classification on CPU"
output: github_document
editor_options: 
  chunk_output_type: console
---

> This tutorial presents an example of application of RNN to text classification using padded data to handle sequences of varying lengths. 

Example based on sentiment analysis on the [IMDB data](http://ai.stanford.edu/~amaas/data/sentiment/).

```{r, echo=FALSE, message=FALSE}
library("readr")
library("dplyr")
library("plotly")
library("stringr")
library("stringi")
library("AUC")
library("scales")
library("mxnet")
library("DiagrammeR")
```


## Data preparation

For this demo, the data preparation is performed by the script `data_preprocessing_seq_to_one.R` which involves the following steps:

- Import IMDB movie reviews  
- Split each review into a word vector and apply some common cleansing (remove special characters, lower case, remove extra blank space...)  
- Convert words into integers and define a dictionary to map the resulting indices with former words  
- Aggregate the buckets of samples and labels into a list

To illustrate the benefit of bucketing, two datasets are created: 

- `corpus_single_train.rds`: no bucketing, all samples are padded/trimmed to 600 words.  
- `corpus_bucketed_train.rds`: samples split into 5 buckets of length 100, 150, 250, 400 and 600. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
corpus_single_train <- readRDS(file = "../data/corpus_single_train.rds")
corpus_single_test <- readRDS(file = "../data/corpus_single_test.rds")

vocab <- length(corpus_single_train$dic)

### Create iterators
batch.size = 64

train.data.single <- mx.io.bucket.iter(buckets = corpus_single_train$buckets, batch.size = batch.size, 
                                       data.mask.element = 0, shuffle = TRUE)

eval.data.single <- mx.io.bucket.iter(buckets = corpus_single_test$buckets, batch.size = batch.size, 
                                      data.mask.element = 0, shuffle = FALSE)
```


## Define the architecture

Below are the graph representations of a seq-to-one architecture with LSTM cells. Note that input data is of shape ` seq.length X batch.size` while the RNN operator requires input of of shape `hidden.features X batch.size X seq.length`, requiring to swap axis. 

For bucketing, a list of symbols is defined, one for each bucket length. During training, at each batch the appropriate symbol is bound according to the bucketID provided by the iterator. 

```{r, echo=TRUE, fig.height=6}
source("../RNN_cells.R")
source("../rnn.graph.dev.R")
seq_len <- 3
symbol_single <- rnn.graph.unroll(seq_len = seq_len, config = "seq-to-one", cell_type = "sru", 
                                  num_rnn_layer = 1, num_embed = 4, num_hidden = 4, 
                                  num_decode = 2, input_size = vocab, dropout = 0.2, 
                                  ignore_label = -1, loss_output = "softmax",
                                  output_last_state = F, masking = T)

symbol_single <- rnn.graph.unroll.straight(seq_len = seq_len, config = "seq-to-one", cell_type = "straight", 
                                           num_rnn_layer = 1, num_embed = 4, num_hidden = 4, 
                                           num_decode = 2, input_size = vocab, dropout = 0.2, 
                                           ignore_label = -1, loss_output = "softmax",
                                           output_last_state = F, masking = T)

symbol_single <- rnn.graph.unroll.rich(seq_len = seq_len, config = "seq-to-one", cell_type = "rich", 
                                       num_rnn_layer = 1, num_embed = 4, num_hidden = 4, 
                                       num_decode = 2, input_size = vocab, dropout = 0.2, 
                                       ignore_label = -1, loss_output = "softmax",
                                       output_last_state = F, masking = T)

graph.viz(symbol_single, type = "graph", direction = "TD", 
          graph.height.px = 800, graph.width.px = 600, shape=c(seq_len, 64))
```


```{r, echo=TRUE, fig.height=1}
source("../RNN_cells.R")
source("../rnn.graph.dev.R")
seq_len <- as.integer(train.data.single$bucket.names)

symbol_single <- rnn.graph.unroll(seq_len = seq_len, config = "seq-to-one", cell_type = "lstm", 
                                  num_rnn_layer = 2, num_embed = 2, num_hidden = 8, 
                                  num_decode = 2, input_size = vocab, dropout = 0, 
                                  ignore_label = -1, loss_output = "softmax",
                                  output_last_state = F, masking = T)

symbol_single <- rnn.graph.unroll.straight(seq_len = seq_len, config = "seq-to-one", cell_type = "straight", 
                                           num_rnn_layer = 2, num_embed = 2, num_hidden = 8, 
                                           num_decode = 2, input_size = vocab, dropout = 0, 
                                           ignore_label = -1, loss_output = "softmax",
                                           output_last_state = F, masking = T)

symbol_single <- rnn.graph.unroll.rich(seq_len = seq_len, config = "seq-to-one", cell_type = "rich", 
                                       num_rnn_layer = 2, num_embed = 2, num_hidden = 8, 
                                       num_decode = 2, input_size = vocab, dropout = 0, 
                                       ignore_label = -1, loss_output = "softmax",
                                       output_last_state = F, masking = T)
```

## Train the model

First the non bucketed model is trained for 8 epochs: 

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
devices <- mx.cpu()

initializer <- mx.init.Xavier(rnd_type = "gaussian", factor_type = "avg", magnitude = 2.5)

# optimizer <- mx.opt.create("rmsprop", learning.rate = 1e-3, gamma1 = 0.95, gamma2 = 0.90,
#                            wd = 1e-5, clip_gradient = 5, rescale.grad=1/batch.size)

optimizer <- mx.opt.create("adam", learning.rate = 5e-4, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8,
                           wd = 1e-5, clip_gradient = 1, rescale.grad=1/batch.size)

# optimizer <- mx.opt.create("adadelta", rho = 0.9, epsilon = 1e-5,
#                            wd = 1e-5, clip_gradient = 1, rescale.grad=1/batch.size)

logger <- mx.metric.logger()
epoch.end.callback <- mx.callback.log.train.metric(period = 1, logger = logger)
batch.end.callback <- mx.callback.log.speedometer(batch.size = batch.size, frequency = 50)

system.time(
  model <- mx.model.buckets(symbol = symbol_single,
                            train.data = train.data.single, eval.data = eval.data.single,
                            num.round = 1, ctx = devices, verbose = TRUE,
                            metric = mx.metric.accuracy, optimizer = optimizer,  
                            initializer = initializer,
                            batch.end.callback = batch.end.callback, 
                            epoch.end.callback = epoch.end.callback)
)

mx.model.save(model, prefix = "../models/nlp_classif_straight", iteration = 0)
```

```{r logger1, echo=FALSE, eval=TRUE, fig.height=4}
plot_ly(x = seq_len(length(logger$train)), y = logger$train, 
        type = "scatter", mode = "markers+lines", name = "train") %>% 
  add_trace(y = logger$eval, type = "scatter", mode = "markers+lines", name = "eval")
```

## Plot word embeddings

Word representation can be visualized by looking at the assigned weights in any of the embedding dimensions. Here, we look simultaneously at the two embeddings learnt in the LSTM model. 

```{r embed, echo=FALSE, eval = TRUE, fig.height=4}
model <- mx.model.load(prefix = "../models/nlp_classif_straight", iteration = 0)

dic <- corpus_single_train$dic
rev_dic <- corpus_single_train$rev_dic

embeddings_weights <- t(as.array(model$arg.params$embed.weight))
embeddings <- data.frame(label=rev_dic, embeddings_weights)
plot_words <- c("terrible", "awesome", "great", "fantastic", "worst", "awful", "pain", "painful", "recommend", "not", "poor", "wonderful", "shame", "ok", "okay", "excellent", "worst", "adequate", "fair", "enjoy", "good", "like", "interesting", "beautiful", "wasn't", "wonderful", "lost", "laugh", "laughable", "unfortunately", "attempt")

embed_ids <- match(plot_words, embeddings$label)
embeddings_sample <- embeddings[embed_ids, ]

p <- ggplot(embeddings_sample, aes(x = X1, y = X2, label = label))
p + geom_text(check_overlap = T, color="navy") + theme_bw() +  theme(panel.grid=element_blank()) + 
  labs(x = "embed 1", y = "embed 2")
```

Since the model attempts to predict the sentiment, it's no surprise that the 2 dimensions into which each word is projected appear correlated with words' polarity. Positive words are associated with lower values along the first embedding (_great_, _excellent_), while the most negative words appear at the far right (_terrible_, _worst_). 

## Inference on test data

The utility function `mx.infer.rnn` has been added to simplify inference on RNN.

```{r, echo=TRUE}
ctx <- mx.cpu(0)
batch.size <- 128

corpus_single_test <- readRDS(file = "../data/corpus_single_test.rds")

test.data <- mx.io.bucket.iter(buckets = corpus_single_test$buckets, batch.size = batch.size, 
                               data.mask.element = 0, shuffle = FALSE)
```


```{r, echo=TRUE, eval=TRUE}
infer <- mx.infer.rnn(infer.data = test.data, model = model, ctx = ctx)

pred_raw <- t(as.array(infer))
pred <- max.col(pred_raw, tie = "first") - 1
label <- unlist(lapply(corpus_single_test$buckets, function(x) x$label))

acc <- sum(label == pred)/length(label)
roc <- roc(predictions = pred_raw[, 2], labels = factor(label))
auc <- auc(roc)
```

Accuracy: `r percent(acc)`  

AUC: `r signif(auc, 4)` 




